{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3aad9119",
      "metadata": {
        "id": "3aad9119"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import tensorflow as tf\n",
        "from skimage.transform import resize\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers, optimizers, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "#from keras.src import layers\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ebf5e13",
      "metadata": {},
      "source": [
        "Get file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4e83a495",
      "metadata": {
        "id": "4e83a495"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Paths\n",
        "image_path_base = r'C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\png'\n",
        "text_path_base = r'C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\myver_cleanCropCMMD.csv'\n",
        "\n",
        "def get_image_paths_and_labels():\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    with open(text_path_base) as f:\n",
        "        f.readline()\n",
        "\n",
        "        for line in f:\n",
        "            splitLine = line.split(\",\")\n",
        "\n",
        "            imagePath = splitLine[2].replace(\"\\n\", \"\").replace('\"',\"\") \n",
        "            image_paths.append(imagePath)\n",
        "\n",
        "            classification = splitLine[1].replace('\"', '') \n",
        "            if classification == \"0\":\n",
        "              labels.append(np.array([0]))\n",
        "            if classification == \"1\":\n",
        "              labels.append(np.array([1]))\n",
        "\n",
        "\n",
        "    return np.array(image_paths), np.array(labels)\n",
        "\n",
        "image_paths, labels = get_image_paths_and_labels()\n",
        "\n",
        "paths_train, paths_test, y_train, y_test = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, random_state=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "5fee431f",
      "metadata": {
        "id": "5fee431f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4472\n",
            "1118\n"
          ]
        }
      ],
      "source": [
        "print(len(paths_train))\n",
        "print(len(paths_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a10cf5da",
      "metadata": {},
      "source": [
        "Data Generator function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0bee961d",
      "metadata": {
        "id": "0bee961d"
      },
      "outputs": [],
      "source": [
        "class DataGenerator:\n",
        "    def __init__(self, image_paths, labels, batch_size=32, augment=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.datagen = ImageDataGenerator(\n",
        "            rotation_range=20,\n",
        "            zoom_range=0.1,\n",
        "            horizontal_flip=True\n",
        "        ) if augment else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_paths = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        batch_images = []\n",
        "        for path in batch_paths:\n",
        "\n",
        "            img = load_img(path, color_mode='rgb')\n",
        "            img_array = img_to_array(img)\n",
        "            img_tensor = tf.convert_to_tensor(img_array)\n",
        "            img = tf.image.resize_with_pad(img_tensor, 600, 600)\n",
        "            batch_images.append(img)\n",
        "\n",
        "        X = np.array(batch_images)\n",
        "        y = np.array(batch_labels).reshape(-1, 1)\n",
        "\n",
        "        if self.augment:\n",
        "            for i in range(len(X)):\n",
        "                if np.random.random() > 0.5: \n",
        "                    X[i] = self.datagen.random_transform(X[i])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def generate(self):\n",
        "        while True:\n",
        "            indices = np.random.permutation(len(self.image_paths))\n",
        "            self.image_paths = self.image_paths[indices]\n",
        "            self.labels = self.labels[indices]\n",
        "\n",
        "            for i in range(len(self)):\n",
        "                yield self.__getitem__(i)\n",
        "\n",
        "train_gen = DataGenerator(paths_train, y_train, batch_size=32, augment=True)\n",
        "test_gen = DataGenerator(paths_test, y_test, batch_size=32, augment=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d70fa0c5",
      "metadata": {},
      "source": [
        "Class weight computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4834e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1]\n",
            "Class distribution: [1125 3347]\n",
            "Class Weights: {0: 1.9875555555555555, 1: 0.6680609501045712}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Flatten y_train into a 1D array\n",
        "y_train_labels = y_train.flatten()\n",
        "\n",
        "print(np.unique(y_train_labels))\n",
        "print(\"Class distribution:\", np.bincount(y_train_labels)) \n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
        "class_weight_dict = dict(zip(np.unique(y_train_labels), class_weights))\n",
        "\n",
        "print(\"Class Weights:\", class_weight_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ee41d52c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pretrained ResNet50\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(600, 600, 3))\n",
        "base_model.trainable = False  # Freeze base initially\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84194283",
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "checkpoint_filepath = r'C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5'\n",
        "chptpt = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d950a8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 1.1437 - accuracy: 0.5195\n",
            "Epoch 1: val_loss improved from inf to 0.96207, saving model to C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5\n",
            "140/140 [==============================] - 852s 6s/step - loss: 1.1437 - accuracy: 0.5195 - val_loss: 0.9621 - val_accuracy: 0.6968 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 1.0046 - accuracy: 0.5812\n",
            "Epoch 2: val_loss improved from 0.96207 to 0.92363, saving model to C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5\n",
            "140/140 [==============================] - 940s 7s/step - loss: 1.0046 - accuracy: 0.5812 - val_loss: 0.9236 - val_accuracy: 0.6565 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.9521 - accuracy: 0.5847\n",
            "Epoch 3: val_loss improved from 0.92363 to 0.88706, saving model to C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5\n",
            "140/140 [==============================] - 1512s 11s/step - loss: 0.9521 - accuracy: 0.5847 - val_loss: 0.8871 - val_accuracy: 0.6646 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.9186 - accuracy: 0.5872\n",
            "Epoch 4: val_loss did not improve from 0.88706\n",
            "140/140 [==============================] - 1429s 10s/step - loss: 0.9186 - accuracy: 0.5872 - val_loss: 0.8948 - val_accuracy: 0.6073 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.8967 - accuracy: 0.5928 \n",
            "Epoch 5: val_loss did not improve from 0.88706\n",
            "140/140 [==============================] - 1731s 12s/step - loss: 0.8967 - accuracy: 0.5928 - val_loss: 0.8956 - val_accuracy: 0.5653 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.8686 - accuracy: 0.6073\n",
            "Epoch 6: val_loss did not improve from 0.88706\n",
            "140/140 [==============================] - 1414s 10s/step - loss: 0.8686 - accuracy: 0.6073 - val_loss: 0.9648 - val_accuracy: 0.4642 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.8546 - accuracy: 0.5901\n",
            "Epoch 7: val_loss improved from 0.88706 to 0.86814, saving model to C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5\n",
            "140/140 [==============================] - 932s 7s/step - loss: 0.8546 - accuracy: 0.5901 - val_loss: 0.8681 - val_accuracy: 0.5948 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.8428 - accuracy: 0.6230 \n",
            "Epoch 8: val_loss improved from 0.86814 to 0.80926, saving model to C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5\n",
            "140/140 [==============================] - 1813s 13s/step - loss: 0.8428 - accuracy: 0.6230 - val_loss: 0.8093 - val_accuracy: 0.6476 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.8411 - accuracy: 0.5941\n",
            "Epoch 9: val_loss improved from 0.80926 to 0.78602, saving model to C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5\n",
            "140/140 [==============================] - 1381s 10s/step - loss: 0.8411 - accuracy: 0.5941 - val_loss: 0.7860 - val_accuracy: 0.6744 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.8305 - accuracy: 0.6136\n",
            "Epoch 10: val_loss did not improve from 0.78602\n",
            "140/140 [==============================] - 1037s 7s/step - loss: 0.8305 - accuracy: 0.6136 - val_loss: 0.8090 - val_accuracy: 0.6530 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.8232 - accuracy: 0.6105\n",
            "Epoch 11: val_loss did not improve from 0.78602\n",
            "140/140 [==============================] - 1278s 9s/step - loss: 0.8232 - accuracy: 0.6105 - val_loss: 0.7910 - val_accuracy: 0.6521 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.8173 - accuracy: 0.6131\n",
            "Epoch 12: val_loss improved from 0.78602 to 0.78046, saving model to C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5\n",
            "140/140 [==============================] - 1134s 8s/step - loss: 0.8173 - accuracy: 0.6131 - val_loss: 0.7805 - val_accuracy: 0.6574 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.8111 - accuracy: 0.6199\n",
            "Epoch 13: val_loss did not improve from 0.78046\n",
            "140/140 [==============================] - 1666s 12s/step - loss: 0.8111 - accuracy: 0.6199 - val_loss: 0.8362 - val_accuracy: 0.5948 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.8042 - accuracy: 0.6127\n",
            "Epoch 14: val_loss did not improve from 0.78046\n",
            "140/140 [==============================] - 830s 6s/step - loss: 0.8042 - accuracy: 0.6127 - val_loss: 0.8123 - val_accuracy: 0.5966 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.7958 - accuracy: 0.6241\n",
            "Epoch 15: val_loss improved from 0.78046 to 0.77101, saving model to C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5\n",
            "140/140 [==============================] - 790s 6s/step - loss: 0.7958 - accuracy: 0.6241 - val_loss: 0.7710 - val_accuracy: 0.6538 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.7942 - accuracy: 0.6279\n",
            "Epoch 16: val_loss did not improve from 0.77101\n",
            "140/140 [==============================] - 797s 6s/step - loss: 0.7942 - accuracy: 0.6279 - val_loss: 0.8341 - val_accuracy: 0.5653 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.7904 - accuracy: 0.6136\n",
            "Epoch 17: val_loss did not improve from 0.77101\n",
            "140/140 [==============================] - 1028s 7s/step - loss: 0.7904 - accuracy: 0.6136 - val_loss: 0.7924 - val_accuracy: 0.6342 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.7850 - accuracy: 0.6205\n",
            "Epoch 18: val_loss did not improve from 0.77101\n",
            "140/140 [==============================] - 970s 7s/step - loss: 0.7850 - accuracy: 0.6205 - val_loss: 0.7772 - val_accuracy: 0.6279 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.7807 - accuracy: 0.6145\n",
            "Epoch 19: val_loss improved from 0.77101 to 0.76656, saving model to C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5\n",
            "140/140 [==============================] - 793s 6s/step - loss: 0.7807 - accuracy: 0.6145 - val_loss: 0.7666 - val_accuracy: 0.6592 - lr: 2.5000e-05\n",
            "Epoch 20/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.7773 - accuracy: 0.6196\n",
            "Epoch 20: val_loss did not improve from 0.76656\n",
            "140/140 [==============================] - 794s 6s/step - loss: 0.7773 - accuracy: 0.6196 - val_loss: 0.7815 - val_accuracy: 0.6109 - lr: 2.5000e-05\n",
            "Epoch 21/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.7734 - accuracy: 0.6299\n",
            "Epoch 21: val_loss did not improve from 0.76656\n",
            "140/140 [==============================] - 799s 6s/step - loss: 0.7734 - accuracy: 0.6299 - val_loss: 0.7924 - val_accuracy: 0.6127 - lr: 2.5000e-05\n",
            "Epoch 22/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.7721 - accuracy: 0.6281\n",
            "Epoch 22: val_loss did not improve from 0.76656\n",
            "140/140 [==============================] - 790s 6s/step - loss: 0.7721 - accuracy: 0.6281 - val_loss: 0.7939 - val_accuracy: 0.5957 - lr: 2.5000e-05\n",
            "Epoch 23/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.7666 - accuracy: 0.6377\n",
            "Epoch 23: val_loss did not improve from 0.76656\n",
            "140/140 [==============================] - 775s 6s/step - loss: 0.7666 - accuracy: 0.6377 - val_loss: 0.7679 - val_accuracy: 0.6369 - lr: 1.2500e-05\n",
            "Epoch 24/100\n",
            "140/140 [==============================] - ETA: 0s - loss: 0.7683 - accuracy: 0.6194\n",
            "Epoch 24: val_loss did not improve from 0.76656\n",
            "140/140 [==============================] - 782s 6s/step - loss: 0.7683 - accuracy: 0.6194 - val_loss: 0.7671 - val_accuracy: 0.6270 - lr: 1.2500e-05\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x24100dd82e0>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_gen.generate(),\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_data=test_gen.generate(),\n",
        "    validation_steps=len(test_gen),\n",
        "    epochs=100,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[early_stop, reduce_lr, chptpt]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "b2f58300",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(r'C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Models\\crop_finished.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "94aa3bea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best weights loaded for evaluation.\n",
            "\n",
            "--- Initial_Model Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.38      0.59      0.46       291\n",
            "   Malignant       0.82      0.66      0.73       827\n",
            "\n",
            "    accuracy                           0.64      1118\n",
            "   macro avg       0.60      0.62      0.59      1118\n",
            "weighted avg       0.70      0.64      0.66      1118\n",
            "\n",
            "Confusion matrix saved to: C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\crop1_confusion_matrix.png\n",
            "ROC curve saved to: C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\crop1_roc_curve.png\n",
            "\n",
            " Final Accuracy: 0.6395, AUC: 0.6538\n",
            "\n",
            "Confusion Matrix:\n",
            "[[172 119]\n",
            " [284 543]]\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load best saved weights\n",
        "model.load_weights(r'C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\weights\\best_model.weights.h5')\n",
        "print(\"Best weights loaded for evaluation.\")\n",
        "\n",
        "def evaluate_model(model, generator, steps, model_name=\"Model\"):\n",
        "    if hasattr(generator, 'on_epoch_end'):\n",
        "        generator.on_epoch_end()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred_prob = []\n",
        "    \n",
        "    for i in range(steps):\n",
        "        if hasattr(generator, '__getitem__'):\n",
        "            batch_x, batch_y = generator.__getitem__(i)\n",
        "        else:\n",
        "            batch_x, batch_y = next(generator.generate())\n",
        "            \n",
        "        batch_pred = model.predict(batch_x, verbose=0)\n",
        "        \n",
        "        y_true.extend(batch_y)\n",
        "        y_pred_prob.extend(batch_pred)\n",
        "\n",
        "    y_true = np.array(y_true).flatten()\n",
        "    y_pred_prob = np.array(y_pred_prob).flatten()\n",
        "    \n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    # --- Classification Report ---\n",
        "    print(f\"\\n--- {model_name} Classification Report ---\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Benign', 'Malignant']))\n",
        "\n",
        "    # --- Confusion Matrix ---\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(2)\n",
        "    plt.xticks(tick_marks, ['Benign', 'Malignant'], rotation=45)\n",
        "    plt.yticks(tick_marks, ['Benign', 'Malignant'])\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                     ha=\"center\", va=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    cm_path = fr'C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\crop1_confusion_matrix.png'\n",
        "    plt.savefig(cm_path)\n",
        "    print(f\"Confusion matrix saved to: {cm_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    # --- ROC Curve ---\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{model_name} ROC Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    \n",
        "    roc_path = fr'C:\\Users\\yaruu\\OneDrive\\Documents\\DIS Copenhagen 2025\\Courses\\ANN & DL\\Final Project\\Datasets\\CMMD_Clean\\crop1_roc_curve.png'\n",
        "    plt.savefig(roc_path)\n",
        "    print(f\"ROC curve saved to: {roc_path}\")\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        'accuracy': (y_pred == y_true).mean(),\n",
        "        'auc': roc_auc,\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_prob': y_pred_prob,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "results = evaluate_model(model, test_gen, len(test_gen), model_name=\"Initial_Model\")\n",
        "\n",
        "print(f\"\\n Final Accuracy: {results['accuracy']:.4f}, AUC: {results['auc']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(results['confusion_matrix'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ann",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
